分類 Classification
================
定義
---------------
* 監督式學習(superived learning)，有標籤
* 從已知的樣本找出特徵後，以此判斷新的樣本屬於已知樣本中的哪一類
  * 舉例：假設原樣本中得出愛吃糖的小孩易蛀牙，新樣本中若看到蛀牙的小孩可以分類為愛吃糖

應用
---------------
* Chum Prediction 流失率預測
* Medical Diagnosis 醫學診斷

分類邊界 Classification Boundaries
---------------
* 分類邊界可能是矩形、平行直線，或其他任何形狀
  ![分類邊界](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex8materials/ex8b_1.png)
  ![分類邊界](http://www.onthelambda.com/wp-content/uploads/2014/07/quadratic.png)

廣泛用來分類的演算法
---------------
* Decision Tree 決策術：機器學習的一種，以樹狀發散的各支點比對對象的屬性值，彼此有映射關係。可用來分析或預測
* K-Nearest Neighbours(KNN) 近鄰演算法：用來分類與回歸的無母數統計法
* Neural Network(NN) 神經網路：機器學習的一種，用來識別圖形、語言
* Support Vector Machines(SVM) 支持向量機：用在非線性分析或線性分析
* Gaussian Mixture Model(GMM) 高斯混合模型：用高斯概率密度函數，將變量分佈量化
* Naive Bayes classifier 樸素貝葉斯方法：貝葉斯方法的簡單化，最大概似估計，為問題分配用特徵值表示的類標籤，類標籤來自有限的集合。特徵之間彼此獨立不關聯
* Radial basis function (RBF)逕向基函數：以變量和原點的距離進行計算
  

Cross Validation 交叉驗證
-------------------
* 將數據樣本切成小子集，在一個小子集（稱為訓練集）上分析，其他小子集（稱為測試集）拿來驗證、確認。目的是使模型在邊訓練邊測試，減少過度擬合<br/>
  ![CrossValidation](http://www.developer.com/imagesvr_ce/6793/ML4.png)
  * 注意：不能拿同個訓練集來測試，這樣效驗結果相近，等於無意義
 
Confusion Matrix 混淆矩陣
-------------------
  ![image](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/60900/versions/13/screenshot.png)<br/>
* 可視化工具，判斷兩個類是否被混淆 
  * TP：正確的肯定(hit)<br/>
TN：正確的否定(correct rejection)<br/>
FP：錯誤的肯定(false alarm)<br/>
FN：錯誤的否定(miss)<br/>
  * True Positive Rate(TPR) 真陽性率 = TP/(TP+FN)
  * True Negative Rate(TNR) 真陰性率(特異度) = TN/(TN+FP)=1-FPR
  * Accuracy(ACC) 準確率 = (TP+TN)/(P+N)<br/>

Receiver Operating Characteristic(ROC曲線)
-------------------
  ![](http://www.mlahanas.de/MOEA/Med/ROC21-Dateien/image001.jpg)<br/>
* 上圖中間的那條線代表預測值，會左右移
* 下圖中主要看上方曲線，值會在線上滑動
  * 如果值靠近右上角，為預值太小(very small threshold)，或過度靠近左下角為預值太大(very large threshold)
  * 中間的虛線代表隨機預測，猜錯猜對機率50%，等於無意義
  * 理論上，上面那條曲線最好能像"厂"(x=100%,y=100%) 
  * 衡量上面那條曲線好不好，從Area Under Curve(AUC)曲線下面積看，面積上限為1，AUC可能是0.8,0.9，越接近1越好
  * 如果AUC連隨機預測50%都不如，這個模型沒有意義 
  <br/><br/>
  ![ROC](https://www.medcalc.org/manual/_help/images/roc_intro3.png)<br/>

Cost Sensitive Learning 代價敏感分類
-------------------
* Cost Sensitive是將錯誤的代價最小化
  * 以上的機器學習的目的是將錯誤率(FN+FP)最小化，沒考慮FN和FP各自佔的比例，這其中關乎成本
  * 舉例：銀行誤判，將錢借給不適合的人的代價(FN)，大於沒將錢借給適合的人(FP)
  * Cost Sensitive目的是降低FN的比例


Lift Analysis 提取分析、Waterfall Analysis 瀑布分析
-------------------
* 從原本的數據集裡，提取較高機率命中的目標對象群
  * 舉例：假設電話營銷，已知有8%的人感興趣，但打電話給全部對象太花時間、成本
  * 於是採取Lift Anaysis(上圖)，為用戶建模，將用戶接受的機率排名打分，前10%的用戶接受度最高，而後曲線漸平緩、效益漸低
  * Waterfall Analysis(下圖)經過打分排序，前10%的人有29%會買該商品。將會買商品的人集中化，就可以精準行銷，減少行銷成本<br/>
  ![](https://i.stack.imgur.com/N1ryb.png)


 
